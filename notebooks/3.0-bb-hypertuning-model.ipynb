{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "# always reload modules marked with \"%aimport\"\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stored data\n",
    "%store -r preprocess_data\n",
    "%store -r group_df\n",
    "%store -r ili_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r reframed\n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_weeks = 5*20*52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = values[:n_train_weeks, :]\n",
    "test = values[n_train_weeks:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5200, 1, 8), (5200,), (5938, 1, 8), (5938,))\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 155\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(1, 8)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/keras/callbacks.py:116: UserWarning: Method on_batch_end() is slow compared to the batch update (0.122381). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 19s, sys: 1min 58s, total: 14min 18s\n",
      "Wall time: 11h 12min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100, 200, 500, 1000]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.774231 using {'epochs': 1000, 'batch_size': 40}\n",
      "0.768269 (0.006861) with: {'epochs': 10, 'batch_size': 10}\n",
      "0.771154 (0.006063) with: {'epochs': 50, 'batch_size': 10}\n",
      "0.773846 (0.007815) with: {'epochs': 100, 'batch_size': 10}\n",
      "0.773077 (0.008776) with: {'epochs': 200, 'batch_size': 10}\n",
      "0.771154 (0.006208) with: {'epochs': 500, 'batch_size': 10}\n",
      "0.735385 (0.024115) with: {'epochs': 1000, 'batch_size': 10}\n",
      "0.768462 (0.007149) with: {'epochs': 10, 'batch_size': 20}\n",
      "0.770769 (0.006025) with: {'epochs': 50, 'batch_size': 20}\n",
      "0.772885 (0.007676) with: {'epochs': 100, 'batch_size': 20}\n",
      "0.773846 (0.008631) with: {'epochs': 200, 'batch_size': 20}\n",
      "0.774038 (0.008505) with: {'epochs': 500, 'batch_size': 20}\n",
      "0.771538 (0.007032) with: {'epochs': 1000, 'batch_size': 20}\n",
      "0.761731 (0.012073) with: {'epochs': 10, 'batch_size': 40}\n",
      "0.769423 (0.006458) with: {'epochs': 50, 'batch_size': 40}\n",
      "0.770769 (0.006387) with: {'epochs': 100, 'batch_size': 40}\n",
      "0.773077 (0.008386) with: {'epochs': 200, 'batch_size': 40}\n",
      "0.772692 (0.008631) with: {'epochs': 500, 'batch_size': 40}\n",
      "0.774231 (0.007543) with: {'epochs': 1000, 'batch_size': 40}\n",
      "0.738077 (0.022375) with: {'epochs': 10, 'batch_size': 60}\n",
      "0.770192 (0.006317) with: {'epochs': 50, 'batch_size': 60}\n",
      "0.770192 (0.006335) with: {'epochs': 100, 'batch_size': 60}\n",
      "0.772308 (0.008115) with: {'epochs': 200, 'batch_size': 60}\n",
      "0.774038 (0.008903) with: {'epochs': 500, 'batch_size': 60}\n",
      "0.773846 (0.007844) with: {'epochs': 1000, 'batch_size': 60}\n",
      "0.630769 (0.014762) with: {'epochs': 10, 'batch_size': 80}\n",
      "0.768654 (0.007405) with: {'epochs': 50, 'batch_size': 80}\n",
      "0.770000 (0.006457) with: {'epochs': 100, 'batch_size': 80}\n",
      "0.769808 (0.006589) with: {'epochs': 200, 'batch_size': 80}\n",
      "0.773654 (0.008359) with: {'epochs': 500, 'batch_size': 80}\n",
      "0.774038 (0.008087) with: {'epochs': 1000, 'batch_size': 80}\n",
      "0.636346 (0.016076) with: {'epochs': 10, 'batch_size': 100}\n",
      "0.767692 (0.008116) with: {'epochs': 50, 'batch_size': 100}\n",
      "0.769231 (0.006606) with: {'epochs': 100, 'batch_size': 100}\n",
      "0.770769 (0.006387) with: {'epochs': 200, 'batch_size': 100}\n",
      "0.774231 (0.007543) with: {'epochs': 500, 'batch_size': 100}\n",
      "0.774231 (0.007573) with: {'epochs': 1000, 'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(1, 8)))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-7f9476248ef1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-7f9476248ef1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    %%time\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "%%time\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
