{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 155\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir,'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r ili_GLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 8 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>ili_activity_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-10-09</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>AK</td>\n",
       "      <td>61.370716</td>\n",
       "      <td>-152.404419</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>AK</td>\n",
       "      <td>61.370716</td>\n",
       "      <td>-152.404419</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-10-23</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>AK</td>\n",
       "      <td>61.370716</td>\n",
       "      <td>-152.404419</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>AK</td>\n",
       "      <td>61.370716</td>\n",
       "      <td>-152.404419</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-11-06</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>AK</td>\n",
       "      <td>61.370716</td>\n",
       "      <td>-152.404419</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  year  month  week state   latitude   longitude  \\\n",
       "0 2010-10-09  2010     10    40    AK  61.370716 -152.404419   \n",
       "1 2010-10-16  2010     10    41    AK  61.370716 -152.404419   \n",
       "2 2010-10-23  2010     10    42    AK  61.370716 -152.404419   \n",
       "3 2010-10-30  2010     10    43    AK  61.370716 -152.404419   \n",
       "4 2010-11-06  2010     10    44    AK  61.370716 -152.404419   \n",
       "\n",
       "  ili_activity_group  \n",
       "0            Minimal  \n",
       "1            Minimal  \n",
       "2            Minimal  \n",
       "3            Minimal  \n",
       "4            Minimal  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(ili_GLL.columns)\n",
    "print(\"The data contains {} features\".format(l))\n",
    "ili_GLL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of location : 46\n"
     ]
    }
   ],
   "source": [
    "nstates = len(ili_GLL.state.unique())\n",
    "print(\"Number of location : {}\".format(nstates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "#from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "#allow to use time distributed content\n",
    "#from keras.layers import TimeDistributed\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "from math import sqrt\n",
    "    \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we put everything together\n",
    "#sepate by states\n",
    "datasets = {}\n",
    "states_label = ili_GLL.state.unique()\n",
    "index = [0]\n",
    "states_label = np.delete(states_label, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'GA', 'HI', 'IA', 'ID',\n",
       "       'IL', 'IN', 'KS', 'KY', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS',\n",
       "       'MT', 'NC', 'ND', 'NE', 'NH', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR',\n",
       "       'PA', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV',\n",
       "       'WY'], dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of location : 45\n"
     ]
    }
   ],
   "source": [
    "for s in states_label:\n",
    "    datasets[s] = ili_GLL[(ili_GLL.state == s)]\n",
    "    datasets[s].drop(['date'], 1, inplace=True)\n",
    "    datasets[s].drop(['state'], 1, inplace=True)\n",
    "\n",
    "nstates = len(datasets)\n",
    "print(\"Number of location : {}\".format(nstates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>ili_activity_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7480</th>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7482</th>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7483</th>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7484</th>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>42.165726</td>\n",
       "      <td>-74.948051</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  week   latitude  longitude ili_activity_group\n",
       "7480  2010     10    40  42.165726 -74.948051            Minimal\n",
       "7481  2010     10    41  42.165726 -74.948051            Minimal\n",
       "7482  2010     10    42  42.165726 -74.948051            Minimal\n",
       "7483  2010     10    43  42.165726 -74.948051            Minimal\n",
       "7484  2010     10    44  42.165726 -74.948051            Minimal"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['NY'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "       Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    print(\"The data contains {} features\".format(n_vars))\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    #n_vars = len(agg.columns)\n",
    "    #print(\"The reframed data contains {} features\".format(n_vars))\n",
    "\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(datasets):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    data_scaled = []\n",
    "    for state in datasets:\n",
    "        scaler = scaler.fit(datasets[state])\n",
    "        #datasets[state] = datasets[state].reshape(datasets[state].shape[0], datasets[state].shape[1])\n",
    "        data_scaled.append(scaler.transform(data_scaled))\n",
    "    return scaler, data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, yhat):\n",
    "    new_row = [x for x in X] + [yhat]\n",
    "    array = numpy.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoder\n",
    "def encode_category(data):\n",
    "    #check the categories\n",
    "    df = pd.get_dummies(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the data into states\n",
    "def parse_data(states_label, datasets, n_weeks = 260):\n",
    "    \"\"\"\n",
    "    return a dictionary\n",
    "    \"\"\"\n",
    "    parse_data = {}\n",
    "    for state in states_label:\n",
    "        data = datasets[state]\n",
    "        names = list(data.columns.values)\n",
    "        \n",
    "        #drop date value\n",
    "        if 'date' in names:\n",
    "            data.drop(['date'], 1, inplace=True)\n",
    "        # make sure that every states has the same number of weeks\n",
    "        if(len(data)>= n_weeks):\n",
    "            parse_data[state] = pd.get_dummies(data)\n",
    "    return parse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reframe_data(parse_data, n_weeks =1,n_features =1 ):\n",
    "    reframed_data = []\n",
    "    for state in parse_data:\n",
    "        values=parse_data[state]\n",
    "        # ensure all data is float\n",
    "        values = values.astype('float32')\n",
    "        # normalize features\n",
    "        reframed = series_to_supervised(values, n_weeks, n_features)\n",
    "        # we are predicting ili activity\n",
    "        reframed_data.append(reframed)\n",
    "    return reframed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-077955635552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# note that the fit method expects a list of callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m my_first_rnn_fitted = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#label for the targeted state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# verbose=0 suppresses the file writing message\n",
    "# note that the fit method expects a list of callbacks\n",
    "start = time.time()\n",
    "my_first_rnn_fitted = model.fit(\n",
    "    train_features,\n",
    "    train_label[0], #label for the targeted state\n",
    "    validation_data= (\n",
    "        test_features,\n",
    "        test_label[0] ),\n",
    "    epochs=2000,\n",
    "    verbose=0,\n",
    "    shuffle = False,\n",
    "    batch_size=52,\n",
    "    callbacks=[checkpoint],\n",
    "    initial_epoch=0\n",
    ")\n",
    "end = time.time()\n",
    "print \"Model took %0.2f seconds to train\"%(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tt_data(datasets, n_total_years = 260, n_train_weeks = 156 ):\n",
    "    train_features, train_label = list(), list()\n",
    "    test_features, test_label = list(), list()\n",
    "    for data in datasets:\n",
    "        values = data.head(n_total_years).values\n",
    "        train = values[:n_train_weeks, :]\n",
    "        test = values[n_train_weeks:, :]\n",
    "        # split into input and outputs\n",
    "        train_X, train_y = train[:, :-1], train[:,-1]\n",
    "        test_X, test_y = test[:, :-1], test[:, -1]\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        train_features.append(train_X)\n",
    "        train_label.append(train_y)\n",
    "        test_features.append(test_X)\n",
    "        test_label.append(test_y)\n",
    "    print(\"number of weeks in a year: {}\".format(n_total_years))\n",
    "    print(\"number of weeks in the training set : {}\".format(n_train_weeks))\n",
    "    return train_features, train_label, test_features, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(labels, refraimed_data, batch_size = 52, n_neurons = 50):\n",
    "    #dataLength =  4 weeks\n",
    "    stateInputs = {}\n",
    "    stateLayers = []\n",
    "    i = 0\n",
    "    for label in labels:\n",
    "        data = refraimed_data[i]\n",
    "        i+=1\n",
    "        timesteps = data.shape[1]\n",
    "        features = data.shape[2]\n",
    "        print(\"timesteps: {}\".format(timesteps))\n",
    "        inputName = \"{}_input\".format(label)\n",
    "        stateInputs[inputName] = Input(shape=(timesteps,features),\n",
    "                                       batch_shape =(batch_size, timesteps, features), \n",
    "                                       name=inputName)\n",
    "        \n",
    "    for state in stateInputs:\n",
    "        stateL = LSTM(n_neurons, return_sequences=False, stateful=True,\n",
    "                            batch_input_shape=(batch_size, timesteps, features))(stateInputs[state])\n",
    "        stateLayers.append(stateL)\n",
    "    #combined the output\n",
    "    output = keras.layers.concatenate(stateLayers)\n",
    "    output = Dense(1, activation='relu', name='wheighthedAverage_output')(output)\n",
    "    stateInput = stateInputs.values()\n",
    "    \n",
    "    model = Model(inputs = stateInput, outputs = [output])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(labels, \n",
    "             train_features, \n",
    "             train_label, \n",
    "             test_features, \n",
    "             test_label , \n",
    "           #  scaler, \n",
    "             nb_epoch, \n",
    "             timesteps = 1, batch_size = 52, n_neurons = 50):\n",
    "    \n",
    "    \n",
    "    # prepare model\n",
    "    model = create_model(labels, train_features)\n",
    "    # fit model\n",
    "    train_rmse, test_rmse = list(), list()\n",
    "    \n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(\n",
    "            train_features,\n",
    "            train_label[0], #label for the targeted state\n",
    "           # validation_data= (\n",
    "           #     test_features,\n",
    "           #     test_label[0] ),\n",
    "            epochs=2000,\n",
    "            verbose=0,\n",
    "            shuffle = False,\n",
    "            batch_size=52,\n",
    "            initial_epoch=0\n",
    "        )        \n",
    "        model.reset_states()\n",
    "        \n",
    "        # evaluate model on train data\n",
    "        # raw_train = raw[-(len(train)+len(test)+1):-len(test)]\n",
    "        train_rmse.append(evaluate(model, train_features, train_label, scaler, 0, batch_size))\n",
    "        model.reset_states()\n",
    "        # evaluate model on test data\n",
    "        #raw_test = raw[-(len(test)+1):]\n",
    "        test_rmse.append(evaluate(model, test_features, test_label, scaler, 0, batch_size))\n",
    "        model.reset_states()\n",
    "    history = DataFrame()\n",
    "    history['train'], history['test'] = train_rmse, test_rmse\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bbuildman/Documents/Developer/GitHub/001-BB-DL-ILI/notebooks/../models'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dir = os.path.join(os.getcwd(),os.pardir, 'models')\n",
    "processed_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on a dataset, returns RMSE in transformed units\n",
    "def evaluate(model,\n",
    "             test_features, \n",
    "             test_label ,\n",
    "             scaler, \n",
    "             offset, \n",
    "             batch_size):\n",
    "    # reshape\n",
    "    reshaped = test_features.reshape(len(test_features), 1, 1)\n",
    "    # forecast dataset\n",
    "    output = model.predict(reshaped, batch_size=batch_size)\n",
    "    # invert data transforms on forecast\n",
    "    predictions = list()\n",
    "    for i in range(len(output)):\n",
    "        yhat = output[i,0]\n",
    "        # invert scaling\n",
    "        # yhat = invert_scale(scaler, X[i], yhat)\n",
    "        # store forecast\n",
    "        predictions.append(yhat)\n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(raw_data[1:], predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run diagnostic experiments\n",
    "def run():\n",
    "    # load dataset\n",
    "    series = parse_data(states_label, datasets, n_weeks = 260)\n",
    "    labels = series.keys()\n",
    "    print labels\n",
    "    # transform data to be supervised learning\n",
    "    reframed = reframe_data(series)\n",
    "    # split data into train and test-sets\n",
    "    train_features, train_label, test_features, test_label = get_tt_data(reframed)\n",
    "    # transform the scale of the data\n",
    "    # scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    # fit and evaluate model\n",
    "    # train_trimmed = train_scaled[2:, :]\n",
    "    # config\n",
    "    repeats = 10\n",
    "    nb_epoch = 10\n",
    "    # run diagnostic tests\n",
    "    for i in range(repeats):\n",
    "        history = fit_lstm(labels, \n",
    "             train_features, \n",
    "             train_label, \n",
    "             test_features, \n",
    "             test_label , \n",
    "         #    scaler, \n",
    "             nb_epoch)\n",
    "        pyplot.plot(history['train'], color='blue')\n",
    "        pyplot.plot(history['test'], color='orange')\n",
    "        print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, history['train'].iloc[-1], history['test'].iloc[-1]))\n",
    "    pyplot.savefig('epochs_diagnostic.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WA', 'WI', 'WV', 'HI', 'TX', 'NE', 'NY', 'PA', 'VA', 'CO', 'CA', 'AL', 'AR', 'IL', 'GA', 'IN', 'AZ', 'CT', 'MD', 'OK', 'OH', 'UT', 'MO', 'MN', 'MT', 'SC', 'KY', 'OR', 'SD']\n",
      "The data contains 10 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 10 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 10 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 10 features\n",
      "The data contains 10 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "The data contains 8 features\n",
      "The data contains 9 features\n",
      "The data contains 10 features\n",
      "The data contains 9 features\n",
      "The data contains 9 features\n",
      "number of weeks in a year: 260\n",
      "number of weeks in the training set : 156\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n",
      "timesteps: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected NE_input to have shape (52, 1, 17) but got array with shape (156, 1, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-9aac22f1ff35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# entry point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-222-0453d90a1093>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m              \u001b[0mtest_label\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m          \u001b[0;31m#    scaler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m              nb_epoch)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-219-4ae41252d0c4>\u001b[0m in \u001b[0;36mfit_lstm\u001b[0;34m(labels, train_features, train_label, test_features, test_label, nb_epoch, timesteps, batch_size, n_neurons)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         )        \n\u001b[1;32m     30\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1408\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bbuildman/anaconda3/envs/python2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected NE_input to have shape (52, 1, 17) but got array with shape (156, 1, 19)"
     ]
    }
   ],
   "source": [
    "# entry point\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.size % batch_size = 0\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
